{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winedf = pd.read_csv('../input/winemag-data_first150k.csv')\n",
    "winedf.head()\n",
    "# remove duplicate\n",
    "winedf[winedf['description'].duplicated(keep = False)].sort_values('description').head(8)\n",
    "winedf = winedf.drop_duplicates('description')\n",
    "# remove missing values\n",
    "winedf=winedf.dropna(subset=['price'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance \n",
    "# catboosting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import Pool, CatBoostRegressor, cv\n",
    "\n",
    "X=winedf.drop(columns=['points'])\n",
    "\n",
    "X=X.fillna(-1)\n",
    "print(X.columns)\n",
    "categorical_features_indices =[0,1, 3,4,5,6,7,8,9,10]\n",
    "y=winedf['points']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=1)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, \n",
    "                                                    random_state=52)\n",
    "# CatBoostRegressor model with Mean squared error loss function.\n",
    "def perform_model(X_train, y_train,X_valid, y_valid,X_test, y_test):\n",
    "    model = CatBoostRegressor(\n",
    "        random_seed = 400,\n",
    "        loss_function = 'RMSE',\n",
    "        iterations=400,\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        cat_features = categorical_features_indices,\n",
    "        eval_set=(X_valid, y_valid),\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(\"RMSE on training data: \"+ model.score(X_train, y_train).astype(str))\n",
    "    print(\"RMSE on test data: \"+ model.score(X_test, y_test).astype(str))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# model evaluation\n",
    "model=perform_model(X_train, y_train,X_valid, y_valid,X_test, y_test)\n",
    "\n",
    "# feature importance plot\n",
    "feature_score = pd.DataFrame(list(zip(X.dtypes.index, model.get_feature_importance(Pool(X, label=y, cat_features=categorical_features_indices)))),\n",
    "                columns=['Feature','Score'])\n",
    "\n",
    "feature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False, kind='quicksort', na_position='last')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,7)\n",
    "ax = feature_score.plot('Feature', 'Score', kind='bar', color='c')\n",
    "ax.set_title(\"Catboost Feature Importance Ranking\", fontsize = 14)\n",
    "ax.set_xlabel('')\n",
    "\n",
    "rects = ax.patches\n",
    "\n",
    "labels = feature_score['Score'].round(2)\n",
    "\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 0.35, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Logistic Regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "models = {}\n",
    "for z in wine:\n",
    "    model = LogisticRegression()\n",
    "    y = y_train == z\n",
    "    model.fit(X_train_dtm, y)\n",
    "    models[z] = model\n",
    "\n",
    "testing_probs = pd.DataFrame(columns = wine)\n",
    "\n",
    "for variety in wine:\n",
    "    testing_probs[variety] = models[variety].predict_proba(X_test_dtm)[:,1]\n",
    "    \n",
    "predicted_wine = testing_probs.idxmax(axis=1)\n",
    "\n",
    "comparison = pd.DataFrame({'actual':y_test.values, 'predicted':predicted_wine.values})   \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy Score:',accuracy_score(comparison.actual, comparison.predicted)*100,\"%\")\n",
    "comparison.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural language processing\n",
    "\n",
    "# vectorize each token in description\n",
    "# define stop words \n",
    "punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(punc)\n",
    "desc = variety_df['description'].values\n",
    "# sklearn's vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
    "X = vectorizer.fit_transform(desc)\n",
    "word_features = vectorizer.get_feature_names()\n",
    "\n",
    "# stemming, vectorizing the token \n",
    "# I am using the SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n",
    "\n",
    "def tokenize(text):\n",
    "    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]\n",
    "\n",
    "vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)\n",
    "X2 = vectorizer2.fit_transform(desc)\n",
    "word_features2 = vectorizer2.get_feature_names()\n",
    "\n",
    "# adjusting the max_features parameter in the tf-idf vectorizer, which selects only the top max_features tokens ordered by their frequencies in the corpus to be included in the vectorizing\n",
    "vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 1000)\n",
    "X3 = vectorizer3.fit_transform(desc)\n",
    "words = vectorizer3.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means\n",
    "# 5 initializations to run faster\n",
    "kmeans = KMeans(n_clusters = 15, n_init = 5, n_jobs = -1)\n",
    "kmeans.fit(X3)\n",
    "\n",
    "# words with the highest frequency in a cluster\n",
    "#  locate the indices of the highest values of the centroid\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
